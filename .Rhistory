plot(result.leaps.forward, scale="adjr2")
plot(result.leaps.forward, scale="bic")
plot(result.leaps.forward, scale="Cp")
# reg_summary <- summary(result.leaps.forward) # get a summary of the models
# plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "b")
#
# # Extract coefficients for the model with, say, 4 predictors
# coef(result.leaps.forward, 4)
# Best subset selection: backward
result.leaps.backward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="backward")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.backward, scale="adjr2")
plot(result.leaps.backward, scale="bic")
plot(result.leaps.backward, scale="Cp")
# Best subset selection: seqrep
# Not sure if this is hybrid
result.leaps.seqrep <- regsubsets(score~., data=HAPPY2, nvmax=10, method="seqrep")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.seqrep, scale="r2")
plot(result.leaps.seqrep, scale="adjr2")
plot(result.leaps.seqrep, scale="bic")
plot(result.leaps.seqrep, scale="Cp")
# Look at last result for AIC and variables to select
step_forward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="forward")
step_backward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="backward")
step_both <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="both")
#Import the csv file and display the first few rows
HAPPY <- read.csv("2019_happiness_cleaned.csv")
# Remove rank, we will use score as the response variable
HAPPY <- subset(HAPPY, select = -c(rank))
head(HAPPY)
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
HAPPY2 <- HAPPY2[ -c(10) ]
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
head(HAPPY2)
HAPPY2
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
HAPPY2 <- HAPPY2[ -c(10) ]
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
head(HAPPY2)
# Model building
null_model <- lm(score~1, HAPPY)
summary(null_model)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model <- lm(score~., HAPPY)
summary(full_model)
null_model2 <- lm(score~1, HAPPY2)
summary(null_model2)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model2 <- lm(score~., HAPPY2)
summary(full_model2)
# Seeing lots of NA's on full model, like for region_Central and Eastern Europe. Why is this?
# This is because we had both the region and the dummy vairables, so we needed to remove one
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
HAPPY2 <- HAPPY2[ -c(`region_Australia and New Zealand`) ]
HAPPY2 <- HAPPY2[ -c(region_Australia and New Zealand) ]
HAPPY2 <- HAPPY2[ -c("region_Australia and New Zealand") ]
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
head(HAPPY2)
HAPPY2 <- HAPPY2[ -c(`region_Australia and New Zealand`) ]
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
HAPPY2 <- HAPPY2[ -c(`region_Australia and New Zealand`) ]
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
HAPPY2 <- HAPPY2[ -c(`region_Australia and New Zealand`) ]
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
HAPPY2 <- HAPPY2[ -c(`region_Australia and New Zealand`) ]
head(HAPPY2)
HAPPY2 <- subset(HAPPY2, select = -c(`region_Australia and New Zealand`))
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
head(HAPPY2)
knitr::opts_chunk$set(echo = TRUE)
#Install and import package to deal with the categorical variable, region.
if (!require(fastDummies)) install.packages('fastDummies', dependencies = TRUE)
library(fastDummies)
if (!require(leaps)) install.packages('leaps', dependencies = TRUE)
library(leaps)
# Modified Box-cox by Dr Sunny Wang. You can minimize this code chunk.
box.cox <- function(x,y,intercept=TRUE, ylim=NULL,
lambda =seq (-1, 1, len=21), transform.x=FALSE,verbose =TRUE, make.plot=TRUE)
{
good.cases <- (y>0)
y <- y[good.cases]
x <- as.matrix(x)
x <- x[good.cases, , drop=F]
g <- exp(mean(log(y)))
if(transform.x)
{
x.pos <- vector(mode= "logical", length=ncol(x))
for(j in 1:ncol(x))
x.pos[j] <- (min(x[j]) > 0)
x.name <- dimnames(x)[[2]]
if( mode(x.name)=="NULL")
x.name <- paste("X",1:ncol(x),sep="")
}
log.lik <- vector(mode ="numeric",length=length(lambda))
for(i in 1:length(lambda))
{
if(lambda[i] !=0)
{
z <- y^lambda[i]
if (transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- x[,j]^lambda[i]
x.new.name[j] <- paste(x.name[j],"^",lambda[i], sep="")
}
}
}
else
{
z<- log(y)
if(transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- log(x[,j])
x.new.name[j] <- paste("log(",x.name[j],")",sep="")
}
}
}
if(transform.x)
{
dimnames(x.new) <- list(NULL,x.new.name)
reg <- lsfit(x.new,z,intercept=intercept)
}
else
reg <- lsfit(x,z,intercept = intercept)
if(verbose)
{ cat("Lambda:",lambda[i], "\n")
ls.print(reg)
}
res <- reg$residuals
ss.res <- sum(res^2)/g^(2.0*(lambda[i] - 1.0 ))
if (lambda[i] !=0.0)
ss.res <- ss.res/lambda[i]^2
log.lik[i] <- -length(y)/2.0*log(ss.res)
}
if(make.plot)
{
if(mode(ylim)=="NULL")
ylim <- range(log.lik)
plot(lambda,log.lik,ylim=ylim, ylab="Log Likelihood", type="b", lty=1)
abline(max(log.lik)-3.8416/2,0,lty=2)
}
return(log.lik)
}
# Mallow's Ck by Dr Sunny Wang
leaps.ck <- function (X, Y, nbest =3)
{
# call leaps using C_k criterion, make plot of C_k versus k,
# and return leaps output in a matrix for nicer output.
# The matrix has two columns: for k and for C_k. The row labels
# contain the regression labels.
leaps.out <- leaps(X,Y,method="Cp",nbest=nbest)
leaps.mat <- cbind(leaps.out$size,round(leaps.out$Cp,digits=4))
colnames(leaps.mat) <- c("k","C_k")
leaps.mat <- cbind(leaps.out$which, leaps.mat)
return(leaps.mat)
}
#Import the csv file and display the first few rows
HAPPY <- read.csv("2019_happiness_cleaned.csv")
# Remove rank, we will use score as the response variable
HAPPY <- subset(HAPPY, select = -c(rank))
head(HAPPY)
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
HAPPY2 <- subset(HAPPY2, select = -c(`region_Australia and New Zealand`))
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
head(HAPPY2)
# Model building
null_model <- lm(score~1, HAPPY)
summary(null_model)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model <- lm(score~., HAPPY)
summary(full_model)
null_model2 <- lm(score~1, HAPPY2)
summary(null_model2)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model2 <- lm(score~., HAPPY2)
summary(full_model2)
# Seeing lots of NA's on full model, like for region_Central and Eastern Europe. Why is this?
# This is because we had both the region and the dummy vairables, so we needed to remove one
# Best subset selection: forward
result.leaps.forward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="forward") # run best subsets selection
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.forward, scale="adjr2")
plot(result.leaps.forward, scale="bic")
plot(result.leaps.forward, scale="Cp")
# reg_summary <- summary(result.leaps.forward) # get a summary of the models
# plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "b")
#
# # Extract coefficients for the model with, say, 4 predictors
# coef(result.leaps.forward, 4)
# Best subset selection: backward
result.leaps.backward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="backward")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.backward, scale="adjr2")
plot(result.leaps.backward, scale="bic")
plot(result.leaps.backward, scale="Cp")
# Best subset selection: seqrep
# Not sure if this is hybrid
result.leaps.seqrep <- regsubsets(score~., data=HAPPY2, nvmax=10, method="seqrep")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.seqrep, scale="r2")
plot(result.leaps.seqrep, scale="adjr2")
plot(result.leaps.seqrep, scale="bic")
plot(result.leaps.seqrep, scale="Cp")
# Look at last result for AIC and variables to select
step_forward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="forward")
step_backward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="backward")
step_both <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="both")
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
head(HAPPY2)
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
head(HAPPY2)
# Model building
null_model <- lm(score~1, HAPPY)
summary(null_model)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model <- lm(score~., HAPPY)
summary(full_model)
null_model2 <- lm(score~1, HAPPY2)
summary(null_model2)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model2 <- lm(score~., HAPPY2)
summary(full_model2)
# Seeing lots of NA's on full model, like for region_Central and Eastern Europe. Why is this?
# This is because we had both the region and the dummy vairables, so we needed to remove one
HAPPY2 <- subset(HAPPY2, select = -c(`region_Western Europe`))
head(HAPPY2)
# Model building
null_model <- lm(score~1, HAPPY)
summary(null_model)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model <- lm(score~., HAPPY)
summary(full_model)
null_model2 <- lm(score~1, HAPPY2)
summary(null_model2)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model2 <- lm(score~., HAPPY2)
summary(full_model2)
# Seeing lots of NA's on full model, like for region_Central and Eastern Europe. Why is this?
# This is because we had both the region and the dummy vairables, so we needed to remove one
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
HAPPY2 <- subset(HAPPY2, select = -c(`region_Western Europe`))
head(HAPPY2)
# Model building
null_model <- lm(score~1, HAPPY)
summary(null_model)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model <- lm(score~., HAPPY)
summary(full_model)
null_model2 <- lm(score~1, HAPPY2)
summary(null_model2)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model2 <- lm(score~., HAPPY2)
summary(full_model2)
# Seeing lots of NA's on full model, like for region_Central and Eastern Europe. Why is this?
# This is because we had both the region and the dummy vairables, so we needed to remove one
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
HAPPY2 <- subset(HAPPY2, select = -c(region))
HAPPY2 <- subset(HAPPY2, select = -c(`region_Australia and New Zealand`))
head(HAPPY2)
# Model building
null_model <- lm(score~1, HAPPY)
summary(null_model)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model <- lm(score~., HAPPY)
summary(full_model)
null_model2 <- lm(score~1, HAPPY2)
summary(null_model2)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model2 <- lm(score~., HAPPY2)
summary(full_model2)
# Seeing lots of NA's on full model, like for region_Central and Eastern Europe. Why is this?
# This is because we had both the region and the dummy vairables, so we needed to remove one
source("~/.active-rstudio-document", echo=TRUE)
date: "2024-12-01"
```{r setup, include=FALSE}
Hello world!
hi
Hello world!
knitr::opts_chunk$set(echo = TRUE)
hi
clear all
clean all
knitr::opts_chunk$set(echo = TRUE)
#Install and import package to deal with the categorical variable, region.
if (!require(fastDummies)) install.packages('fastDummies', dependencies = TRUE)
library(fastDummies)
if (!require(leaps)) install.packages('leaps', dependencies = TRUE)
library(leaps)
# Modified Box-cox by Dr Sunny Wang. You can minimize this code chunk.
box.cox <- function(x,y,intercept=TRUE, ylim=NULL,
lambda =seq (-1, 1, len=21), transform.x=FALSE,verbose =TRUE, make.plot=TRUE)
{
good.cases <- (y>0)
y <- y[good.cases]
x <- as.matrix(x)
x <- x[good.cases, , drop=F]
g <- exp(mean(log(y)))
if(transform.x)
{
x.pos <- vector(mode= "logical", length=ncol(x))
for(j in 1:ncol(x))
x.pos[j] <- (min(x[j]) > 0)
x.name <- dimnames(x)[[2]]
if( mode(x.name)=="NULL")
x.name <- paste("X",1:ncol(x),sep="")
}
log.lik <- vector(mode ="numeric",length=length(lambda))
for(i in 1:length(lambda))
{
if(lambda[i] !=0)
{
z <- y^lambda[i]
if (transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- x[,j]^lambda[i]
x.new.name[j] <- paste(x.name[j],"^",lambda[i], sep="")
}
}
}
else
{
z<- log(y)
if(transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- log(x[,j])
x.new.name[j] <- paste("log(",x.name[j],")",sep="")
}
}
}
if(transform.x)
{
dimnames(x.new) <- list(NULL,x.new.name)
reg <- lsfit(x.new,z,intercept=intercept)
}
else
reg <- lsfit(x,z,intercept = intercept)
if(verbose)
{ cat("Lambda:",lambda[i], "\n")
ls.print(reg)
}
res <- reg$residuals
ss.res <- sum(res^2)/g^(2.0*(lambda[i] - 1.0 ))
if (lambda[i] !=0.0)
ss.res <- ss.res/lambda[i]^2
log.lik[i] <- -length(y)/2.0*log(ss.res)
}
if(make.plot)
{
if(mode(ylim)=="NULL")
ylim <- range(log.lik)
plot(lambda,log.lik,ylim=ylim, ylab="Log Likelihood", type="b", lty=1)
abline(max(log.lik)-3.8416/2,0,lty=2)
}
return(log.lik)
}
# Mallow's Ck by Dr Sunny Wang
leaps.ck <- function (X, Y, nbest =3)
{
# call leaps using C_k criterion, make plot of C_k versus k,
# and return leaps output in a matrix for nicer output.
# The matrix has two columns: for k and for C_k. The row labels
# contain the regression labels.
leaps.out <- leaps(X,Y,method="Cp",nbest=nbest)
leaps.mat <- cbind(leaps.out$size,round(leaps.out$Cp,digits=4))
colnames(leaps.mat) <- c("k","C_k")
leaps.mat <- cbind(leaps.out$which, leaps.mat)
return(leaps.mat)
}
#Import the csv file
HAPPY <- read.csv("2019_happiness_cleaned.csv")
# Remove rank, as we will be using score as the response variable
HAPPY <- subset(HAPPY, select = -c(rank))
head(HAPPY)
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
# Drop the`Australia and New Zealand` region, because we need to ensure t(X) %*% X will be non-singular.
HAPPY2 <- subset(HAPPY2, select = -c(region, `region_Australia and New Zealand`))
head(HAPPY2)
# Model building
null_model2 <- lm(score~1, HAPPY2)
summary(null_model2)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model2 <- lm(score~., HAPPY2)
summary(full_model2)
# Seeing lots of NA's on full model, like for region_Central and Eastern Europe. Why is this?
# This is because we had both the region and the dummy vairables, so we needed to remove one.
# Best subset selection: forward
result.leaps.forward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="forward") # run best subsets selection
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.forward, scale="adjr2")
plot(result.leaps.forward, scale="bic")
plot(result.leaps.forward, scale="Cp")
# reg_summary <- summary(result.leaps.forward) # get a summary of the models
# plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "b")
#
# # Extract coefficients for the model with, say, 4 predictors
# coef(result.leaps.forward, 4)
# Best subset selection: backward
result.leaps.backward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="backward")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.backward, scale="adjr2")
plot(result.leaps.backward, scale="bic")
plot(result.leaps.backward, scale="Cp")
# Best subset selection: seqrep
# Not sure if this is hybrid
result.leaps.seqrep <- regsubsets(score~., data=HAPPY2, nvmax=10, method="seqrep")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.seqrep, scale="r2")
plot(result.leaps.seqrep, scale="adjr2")
plot(result.leaps.seqrep, scale="bic")
plot(result.leaps.seqrep, scale="Cp")
# Look at last result for AIC and variables to select
step_forward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="forward")
step_backward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="backward")
step_both <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="both")
ISITTRUE <- HAPPY$score / (rowSums(HAPPY))
ISITTRUE <- HAPPY$score / (rowSums(as.matrix(HAPPY)))
ISITTRUE <- HAPPY$score / (rowSums(as.matrix(HAPPY[,2:])))
ISITTRUE <- HAPPY$score / (rowSums(as.matrix(HAPPY[,c(2:ncol(HAPPY))])))
ISITTRUE
(rowSums(as.matrix(HAPPY[,c(2:ncol(HAPPY))])))
ISITTRUE <- HAPPY$score / (rowSums(as.matrix(HAPPY[,c(3:ncol(HAPPY))])))
(rowSums(as.matrix(HAPPY[,c(2:ncol(HAPPY))])))
ISITTRUE
HAPPY[,c(3:ncol(HAPPY))]
HAPPY[,c(1:ncol(HAPPY))]
ISITTRUE <- HAPPY$score - (rowSums(as.matrix(HAPPY[,c(3:ncol(HAPPY))])))
ISITTRUE
knitr::opts_chunk$set(echo = TRUE)
#Import the csv file
HAPPY <- read.csv("2019_happiness_cleaned.csv")
# Remove rank, as we will be using score as the response variable
HAPPY <- subset(HAPPY, select = -c(rank))
head(HAPPY)
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
# Drop the`Australia and New Zealand` region, because we need to ensure t(X) %*% X will be non-singular.
HAPPY2 <- subset(HAPPY2, select = -c(region, `region_Australia and New Zealand`))
head(HAPPY2)
X <- cbind(1, as.matrix(HAPPY[c(,3:)]))
X <- cbind(1, as.matrix(HAPPY[c(,3:ncol(HAPPY))]))
X <- cbind(1, as.matrix( HAPPY[,c(3:ncol(HAPPY))] ))
X
head(X)
X <- cbind(1, as.matrix( HAPPY2[,c(2:ncol(HAPPY2))] ))
head(X)
Y <- cbind(as.matrix( HAPPY2[,c(,1)] ))
head(X)
Y <- cbind(as.matrix( HAPPY2[,c(,1)] ))
Y <- cbind(as.matrix( HAPPY2[,c(1)] ))
head(X)
head(Y)
