ls.part.res.plot(X[,2:ncol(X)],Y,4)
ls.added.var.plot(X[,2:ncol(X)],Y,4)
plot(X[,6], happy.res.before, xlab="Generosity", ylab="residuals", main="Generosity Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,5)
ls.added.var.plot(X[,2:ncol(X)],Y,5)
plot(X[,7], happy.res.before, xlab="Corruption", ylab="residuals", main="Corruption Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,6)
ls.added.var.plot(X[,2:ncol(X)],Y,6)
happy.lm <- lm(score~.,data=HAPPY2)
happy.res.before <- happy.lm$residuals
par(mfrow=c(2,2))
plot(X[,2], happy.res.before, xlab="GDP", ylab="residuals", main="GDP Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,1)
ls.added.var.plot(X[,2:ncol(X)],Y,1)
plot(X[,3], happy.res.before, xlab="Support", ylab="residuals", main="Support Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,2)
ls.added.var.plot(X[,2:ncol(X)],Y,2)
plot(X[,4], happy.res.before, xlab="Life Expectancy", ylab="residuals",main="Life Expectancy Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,3)
ls.added.var.plot(X[,2:ncol(X)],Y,3)
plot(X[,5], happy.res.before, xlab="Freedom", ylab="residuals", main="Freedom Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,4)
ls.added.var.plot(X[,2:ncol(X)],Y,4)
plot(X[,6], happy.res.before, xlab="Generosity", ylab="residuals", main="Generosity Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,5)
ls.added.var.plot(X[,2:ncol(X)],Y,5)
plot(X[,7], happy.res.before, xlab="Corruption", ylab="residuals", main="Corruption Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,6)
ls.added.var.plot(X[,2:ncol(X)],Y,6)
happy.lm <- lm(score~.,data=HAPPY2)
happy.res.before <- happy.lm$residuals
par(mfrow=c(3,2))
plot(X[,2], happy.res.before, xlab="GDP", ylab="residuals", main="GDP Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,1)
ls.added.var.plot(X[,2:ncol(X)],Y,1)
plot(X[,3], happy.res.before, xlab="Support", ylab="residuals", main="Support Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,2)
ls.added.var.plot(X[,2:ncol(X)],Y,2)
plot(X[,4], happy.res.before, xlab="Life Expectancy", ylab="residuals",main="Life Expectancy Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,3)
ls.added.var.plot(X[,2:ncol(X)],Y,3)
plot(X[,5], happy.res.before, xlab="Freedom", ylab="residuals", main="Freedom Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,4)
ls.added.var.plot(X[,2:ncol(X)],Y,4)
plot(X[,6], happy.res.before, xlab="Generosity", ylab="residuals", main="Generosity Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,5)
ls.added.var.plot(X[,2:ncol(X)],Y,5)
plot(X[,7], happy.res.before, xlab="Corruption", ylab="residuals", main="Corruption Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,6)
ls.added.var.plot(X[,2:ncol(X)],Y,6)
happy.lm <- lm(score~.,data=HAPPY2)
happy.res.before <- happy.lm$residuals
par(mfrow=c(2,3))
plot(X[,2], happy.res.before, xlab="GDP", ylab="residuals", main="GDP Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,1)
ls.added.var.plot(X[,2:ncol(X)],Y,1)
plot(X[,3], happy.res.before, xlab="Support", ylab="residuals", main="Support Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,2)
ls.added.var.plot(X[,2:ncol(X)],Y,2)
plot(X[,4], happy.res.before, xlab="Life Expectancy", ylab="residuals",main="Life Expectancy Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,3)
ls.added.var.plot(X[,2:ncol(X)],Y,3)
plot(X[,5], happy.res.before, xlab="Freedom", ylab="residuals", main="Freedom Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,4)
ls.added.var.plot(X[,2:ncol(X)],Y,4)
plot(X[,6], happy.res.before, xlab="Generosity", ylab="residuals", main="Generosity Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,5)
ls.added.var.plot(X[,2:ncol(X)],Y,5)
plot(X[,7], happy.res.before, xlab="Corruption", ylab="residuals", main="Corruption Residual Plot")
ls.part.res.plot(X[,2:ncol(X)],Y,6)
ls.added.var.plot(X[,2:ncol(X)],Y,6)
knitr::opts_chunk$set(echo = TRUE)
#Install and import package to deal with the categorical variable, region.
if (!require(fastDummies)) install.packages('fastDummies', dependencies = TRUE)
library(fastDummies)
if (!require(leaps)) install.packages('leaps', dependencies = TRUE)
library(leaps)
# Modified Box-cox by Dr Sunny Wang. You can minimize this code chunk.
box.cox <- function(x,y,intercept=TRUE, ylim=NULL,
lambda =seq (-1, 1, len=21), transform.x=FALSE,verbose =TRUE, make.plot=TRUE)
{
good.cases <- (y>0)
y <- y[good.cases]
x <- as.matrix(x)
x <- x[good.cases, , drop=F]
g <- exp(mean(log(y)))
if(transform.x)
{
x.pos <- vector(mode= "logical", length=ncol(x))
for(j in 1:ncol(x))
x.pos[j] <- (min(x[j]) > 0)
x.name <- dimnames(x)[[2]]
if( mode(x.name)=="NULL")
x.name <- paste("X",1:ncol(x),sep="")
}
log.lik <- vector(mode ="numeric",length=length(lambda))
for(i in 1:length(lambda))
{
if(lambda[i] !=0)
{
z <- y^lambda[i]
if (transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- x[,j]^lambda[i]
x.new.name[j] <- paste(x.name[j],"^",lambda[i], sep="")
}
}
}
else
{
z<- log(y)
if(transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- log(x[,j])
x.new.name[j] <- paste("log(",x.name[j],")",sep="")
}
}
}
if(transform.x)
{
dimnames(x.new) <- list(NULL,x.new.name)
reg <- lsfit(x.new,z,intercept=intercept)
}
else
reg <- lsfit(x,z,intercept = intercept)
if(verbose)
{ cat("Lambda:",lambda[i], "\n")
ls.print(reg)
}
res <- reg$residuals
ss.res <- sum(res^2)/g^(2.0*(lambda[i] - 1.0 ))
if (lambda[i] !=0.0)
ss.res <- ss.res/lambda[i]^2
log.lik[i] <- -length(y)/2.0*log(ss.res)
}
if(make.plot)
{
if(mode(ylim)=="NULL")
ylim <- range(log.lik)
plot(lambda,log.lik,ylim=ylim, ylab="Log Likelihood", type="b", lty=1)
abline(max(log.lik)-3.8416/2,0,lty=2)
}
return(log.lik)
}
# Mallow's Ck by Dr Sunny Wang
leaps.ck <- function (X, Y, nbest =3)
{
# call leaps using C_k criterion, make plot of C_k versus k,
# and return leaps output in a matrix for nicer output.
# The matrix has two columns: for k and for C_k. The row labels
# contain the regression labels.
leaps.out <- leaps(X,Y,method="Cp",nbest=nbest)
leaps.mat <- cbind(leaps.out$size,round(leaps.out$Cp,digits=4))
colnames(leaps.mat) <- c("k","C_k")
leaps.mat <- cbind(leaps.out$which, leaps.mat)
return(leaps.mat)
}
#Import the csv file
HAPPY <- read.csv("2019_happiness_cleaned.csv")
# Remove rank, as we will be using score as the response variable
HAPPY <- subset(HAPPY, select = -c(rank))
head(HAPPY)
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
# Drop the`Australia and New Zealand` region, because we need to ensure t(X) %*% X will be non-singular.
HAPPY2 <- subset(HAPPY2, select = -c(region, `region_Australia and New Zealand`))
head(HAPPY2)
# Model building
null_model2 <- lm(score~1, HAPPY2)
summary(null_model2)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model2 <- lm(score~., HAPPY2)
summary(full_model2)
# Best subset selection: forward
result.leaps.forward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="forward") # run best subsets selection
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.forward, scale="adjr2")
plot(result.leaps.forward, scale="bic")
plot(result.leaps.forward, scale="Cp")
# reg_summary <- summary(result.leaps.forward) # get a summary of the models
# plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "b")
#
# # Extract coefficients for the model with, say, 4 predictors
# coef(result.leaps.forward, 4)
# Best subset selection: backward
result.leaps.backward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="backward")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.backward, scale="adjr2")
plot(result.leaps.backward, scale="bic")
plot(result.leaps.backward, scale="Cp")
# Best subset selection: seqrep
# Not sure if this is hybrid
result.leaps.seqrep <- regsubsets(score~., data=HAPPY2, nvmax=10, method="seqrep")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.seqrep, scale="r2")
plot(result.leaps.seqrep, scale="adjr2")
plot(result.leaps.seqrep, scale="bic")
plot(result.leaps.seqrep, scale="Cp")
# Look at last result for AIC and variables to select
step_forward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="forward")
knitr::opts_chunk$set(echo = TRUE)
#Install and import package to deal with the categorical variable, region.
if (!require(fastDummies)) install.packages('fastDummies', dependencies = TRUE)
library(fastDummies)
if (!require(leaps)) install.packages('leaps', dependencies = TRUE)
library(leaps)
# Modified Box-cox by Dr Sunny Wang. You can minimize this code chunk.
box.cox <- function(x,y,intercept=TRUE, ylim=NULL,
lambda =seq (-1, 1, len=21), transform.x=FALSE,verbose =TRUE, make.plot=TRUE)
{
good.cases <- (y>0)
y <- y[good.cases]
x <- as.matrix(x)
x <- x[good.cases, , drop=F]
g <- exp(mean(log(y)))
if(transform.x)
{
x.pos <- vector(mode= "logical", length=ncol(x))
for(j in 1:ncol(x))
x.pos[j] <- (min(x[j]) > 0)
x.name <- dimnames(x)[[2]]
if( mode(x.name)=="NULL")
x.name <- paste("X",1:ncol(x),sep="")
}
log.lik <- vector(mode ="numeric",length=length(lambda))
for(i in 1:length(lambda))
{
if(lambda[i] !=0)
{
z <- y^lambda[i]
if (transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- x[,j]^lambda[i]
x.new.name[j] <- paste(x.name[j],"^",lambda[i], sep="")
}
}
}
else
{
z<- log(y)
if(transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- log(x[,j])
x.new.name[j] <- paste("log(",x.name[j],")",sep="")
}
}
}
if(transform.x)
{
dimnames(x.new) <- list(NULL,x.new.name)
reg <- lsfit(x.new,z,intercept=intercept)
}
else
reg <- lsfit(x,z,intercept = intercept)
if(verbose)
{ cat("Lambda:",lambda[i], "\n")
ls.print(reg)
}
res <- reg$residuals
ss.res <- sum(res^2)/g^(2.0*(lambda[i] - 1.0 ))
if (lambda[i] !=0.0)
ss.res <- ss.res/lambda[i]^2
log.lik[i] <- -length(y)/2.0*log(ss.res)
}
if(make.plot)
{
if(mode(ylim)=="NULL")
ylim <- range(log.lik)
plot(lambda,log.lik,ylim=ylim, ylab="Log Likelihood", type="b", lty=1)
abline(max(log.lik)-3.8416/2,0,lty=2)
}
return(log.lik)
}
# Mallow's Ck by Dr Sunny Wang
leaps.ck <- function (X, Y, nbest =3)
{
# call leaps using C_k criterion, make plot of C_k versus k,
# and return leaps output in a matrix for nicer output.
# The matrix has two columns: for k and for C_k. The row labels
# contain the regression labels.
leaps.out <- leaps(X,Y,method="Cp",nbest=nbest)
leaps.mat <- cbind(leaps.out$size,round(leaps.out$Cp,digits=4))
colnames(leaps.mat) <- c("k","C_k")
leaps.mat <- cbind(leaps.out$which, leaps.mat)
return(leaps.mat)
}
#Import the csv file
HAPPY <- read.csv("2019_happiness_cleaned.csv")
# Remove rank, as we will be using score as the response variable
HAPPY <- subset(HAPPY, select = -c(rank))
head(HAPPY)
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
# Drop the`Australia and New Zealand` region, because we need to ensure t(X) %*% X will be non-singular.
HAPPY2 <- subset(HAPPY2, select = -c(region, `region_Australia and New Zealand`))
head(HAPPY2)
# Model building
null_model <- lm(score~1, HAPPY2)
summary(null_model)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model <- lm(score~., HAPPY2)
summary(full_model)
# Best subset selection: forward
result.leaps.forward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="forward") # run best subsets selection
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.forward, scale="adjr2")
plot(result.leaps.forward, scale="bic")
plot(result.leaps.forward, scale="Cp")
# reg_summary <- summary(result.leaps.forward) # get a summary of the models
# plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "b")
#
# # Extract coefficients for the model with, say, 4 predictors
# coef(result.leaps.forward, 4)
# Best subset selection: backward
result.leaps.backward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="backward")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.backward, scale="adjr2")
plot(result.leaps.backward, scale="bic")
plot(result.leaps.backward, scale="Cp")
# Best subset selection: seqrep
# Not sure if this is hybrid
result.leaps.seqrep <- regsubsets(score~., data=HAPPY2, nvmax=10, method="seqrep")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.seqrep, scale="r2")
plot(result.leaps.seqrep, scale="adjr2")
plot(result.leaps.seqrep, scale="bic")
plot(result.leaps.seqrep, scale="Cp")
# Look at last result for AIC and variables to select
step_forward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="forward")
step_backward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="backward")
step_both <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="both")
#Install and import package to deal with the categorical variable, region.
if (!require(fastDummies)) install.packages('fastDummies', dependencies = TRUE)
library(fastDummies)
if (!require(leaps)) install.packages('leaps', dependencies = TRUE)
library(leaps)
# Modified Box-cox by Dr Sunny Wang. You can minimize this code chunk.
box.cox <- function(x,y,intercept=TRUE, ylim=NULL,
lambda =seq (-1, 1, len=21), transform.x=FALSE,verbose =TRUE, make.plot=TRUE)
{
good.cases <- (y>0)
y <- y[good.cases]
x <- as.matrix(x)
x <- x[good.cases, , drop=F]
g <- exp(mean(log(y)))
if(transform.x)
{
x.pos <- vector(mode= "logical", length=ncol(x))
for(j in 1:ncol(x))
x.pos[j] <- (min(x[j]) > 0)
x.name <- dimnames(x)[[2]]
if( mode(x.name)=="NULL")
x.name <- paste("X",1:ncol(x),sep="")
}
log.lik <- vector(mode ="numeric",length=length(lambda))
for(i in 1:length(lambda))
{
if(lambda[i] !=0)
{
z <- y^lambda[i]
if (transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- x[,j]^lambda[i]
x.new.name[j] <- paste(x.name[j],"^",lambda[i], sep="")
}
}
}
else
{
z<- log(y)
if(transform.x)
{
x.new <- x
x.new.name <- x.name
for(j in 1:ncol(x))
if(x.pos[j])
{
x.new[,j] <- log(x[,j])
x.new.name[j] <- paste("log(",x.name[j],")",sep="")
}
}
}
if(transform.x)
{
dimnames(x.new) <- list(NULL,x.new.name)
reg <- lsfit(x.new,z,intercept=intercept)
}
else
reg <- lsfit(x,z,intercept = intercept)
if(verbose)
{ cat("Lambda:",lambda[i], "\n")
ls.print(reg)
}
res <- reg$residuals
ss.res <- sum(res^2)/g^(2.0*(lambda[i] - 1.0 ))
if (lambda[i] !=0.0)
ss.res <- ss.res/lambda[i]^2
log.lik[i] <- -length(y)/2.0*log(ss.res)
}
if(make.plot)
{
if(mode(ylim)=="NULL")
ylim <- range(log.lik)
plot(lambda,log.lik,ylim=ylim, ylab="Log Likelihood", type="b", lty=1)
abline(max(log.lik)-3.8416/2,0,lty=2)
}
return(log.lik)
}
# Mallow's Ck by Dr Sunny Wang
leaps.ck <- function (X, Y, nbest =3)
{
# call leaps using C_k criterion, make plot of C_k versus k,
# and return leaps output in a matrix for nicer output.
# The matrix has two columns: for k and for C_k. The row labels
# contain the regression labels.
leaps.out <- leaps(X,Y,method="Cp",nbest=nbest)
leaps.mat <- cbind(leaps.out$size,round(leaps.out$Cp,digits=4))
colnames(leaps.mat) <- c("k","C_k")
leaps.mat <- cbind(leaps.out$which, leaps.mat)
return(leaps.mat)
}
#Import the csv file
HAPPY <- read.csv("2019_happiness_cleaned.csv")
# Remove rank, as we will be using score as the response variable
HAPPY <- subset(HAPPY, select = -c(rank))
head(HAPPY)
#Create dummy variables
HAPPY2 <- dummy_cols(HAPPY, select_columns = "region")
# Drop the `region`, because we have converted it into dummy variables
# Drop the`Australia and New Zealand` region, because we need to ensure t(X) %*% X will be non-singular.
HAPPY2 <- subset(HAPPY2, select = -c(region, `region_Australia and New Zealand`))
head(HAPPY2)
# Model building
null_model <- lm(score~1, HAPPY2)
summary(null_model)
# Currently: Not an optimal model. Needs to add interaction effects.
full_model <- lm(score~., HAPPY2)
summary(full_model)
# Best subset selection: forward
result.leaps.forward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="forward") # run best subsets selection
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.forward, scale="adjr2")
plot(result.leaps.forward, scale="bic")
plot(result.leaps.forward, scale="Cp")
# reg_summary <- summary(result.leaps.forward) # get a summary of the models
# plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "b")
#
# # Extract coefficients for the model with, say, 4 predictors
# coef(result.leaps.forward, 4)
# Best subset selection: backward
result.leaps.backward <- regsubsets(score~., data=HAPPY2, nvmax=10, method="backward")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.forward, scale="r2")
plot(result.leaps.backward, scale="adjr2")
plot(result.leaps.backward, scale="bic")
plot(result.leaps.backward, scale="Cp")
# Best subset selection: seqrep
# Not sure if this is hybrid
result.leaps.seqrep <- regsubsets(score~., data=HAPPY2, nvmax=10, method="seqrep")
# Maybe ignore R^2 for now, not preferable criteria
# Note that AIC isn't a thing for this
plot(result.leaps.seqrep, scale="r2")
plot(result.leaps.seqrep, scale="adjr2")
plot(result.leaps.seqrep, scale="bic")
plot(result.leaps.seqrep, scale="Cp")
# Look at last result for AIC and variables to select
step_forward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="forward")
step_backward <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="backward")
step_both <- step(null_model, scope=list(lower=null_model, upper=full_model), direction="both")
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE)
tinytex::install_tinytex()
